{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from copy import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from spektral.datasets import mnist\n",
    "from spektral.layers import GraphConv\n",
    "from spektral.layers.ops import sp_matrix_to_sp_tensor\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Parameters\n",
    "l2_reg = 5e-4         # Regularization rate for l2\n",
    "learning_rate = 1e-3  # Learning rate for SGD\n",
    "batch_size = 32       # Batch size\n",
    "epochs = 1000       # Number of training epochs\n",
    "es_patience = 10      # Patience fot early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "class GTZANGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=BATCH_SIZE, is_test = False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indexes\n",
    "        signals = self.X[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Apply data augmentation\n",
    "        if not self.is_test:\n",
    "            pass\n",
    "            #signals = self.__augment(signals)\n",
    "            \n",
    "        return signals, self.y[index*self.batch_size:(index+1)*self.batch_size]\n",
    "    \n",
    "    def __augment(self, signals, hor_flip = 0.5, random_cutout = 0.5):\n",
    "        spectrograms =  []\n",
    "        for s in signals:\n",
    "            signal = copy(s)\n",
    "        \n",
    "            # Perform random cutoout of some frequency/time\n",
    "            if np.random.rand() < random_cutout:\n",
    "                lines = np.random.randint(signal.shape[0], size=3)\n",
    "                cols = np.random.randint(signal.shape[0], size=4)\n",
    "                signal[lines, :, :] = -80 # dB\n",
    "                signal[:, cols, :] = -80 # dB\n",
    "\n",
    "            spectrograms.append(signal)\n",
    "        return np.array(spectrograms)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# majority vote\n",
    "def majority_vote(scores):\n",
    "    values, counts = np.unique(scores,return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    return values[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "Loading dataset...\n",
      "============================================================================\n",
      "(12160, 128, 1)\n",
      "(3040, 128, 1)\n",
      "(3800, 128, 1)\n",
      "(12160, 10)\n",
      "(3040, 10)\n",
      "(3800, 10)\n",
      "(128, 128)\n",
      "============================================================================\n",
      "complete!\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print('============================================================================')\n",
    "print('Loading dataset...')\n",
    "print('============================================================================')\n",
    "X_train = np.load('/home/jaehwlee/Genre_classification/GNN/feature_data/X_train_features.npy')\n",
    "X_valid = np.load('/home/jaehwlee/Genre_classification/GNN/feature_data/X_valid_features.npy')\n",
    "X_test = np.load('/home/jaehwlee/Genre_classification/GNN/feature_data/X_test_features.npy')\n",
    "y_train = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/y_train.npy')\n",
    "y_valid = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/y_valid.npy')\n",
    "y_test = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/y_test.npy')\n",
    "\n",
    "A = np.ones((128,128))\n",
    "\n",
    "X_train, X_valid, X_test = X_train[..., None], X_valid[..., None], X_test[..., None]\n",
    "\n",
    "song_samples = 660000\n",
    "genres = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4, \n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "print('============================================================================')\n",
    "print('complete!')\n",
    "print('============================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_model():\n",
    "    # Model definition\n",
    "    N = X_train.shape[-2]\n",
    "    F = X_train.shape[-1]\n",
    "\n",
    "    n_out = 10\n",
    "\n",
    "    fltr = GraphConv.preprocess(A)\n",
    "    X_in = Input(shape=(N, F))\n",
    "    # Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n",
    "    # different rank.\n",
    "    A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "    graph_conv = GraphConv(32,\n",
    "                           activation='elu',\n",
    "                           kernel_regularizer=l2(l2_reg))([X_in, A_in])\n",
    "    graph_conv = GraphConv(32,\n",
    "                           activation='elu',\n",
    "                           kernel_regularizer=l2(l2_reg))([graph_conv, A_in])\n",
    "    flatten = Flatten()(graph_conv)\n",
    "    fc = Dense(512, activation='relu')(flatten)\n",
    "    output = Dense(n_out, activation='softmax')(fc)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = GTZANGenerator(X_train, y_train)\n",
    "steps_per_epoch = np.ceil(len(X_train)/batch_size)\n",
    "\n",
    "validation_generator = GTZANGenerator(X_valid, y_valid)\n",
    "val_steps = np.ceil(len(X_test)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer graph_conv_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer graph_conv_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 128, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(128, 128)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_6 (GraphConv)        (None, 128, 32)      64          input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_7 (GraphConv)        (None, 128, 32)      1056        graph_conv_6[0][0]               \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4096)         0           graph_conv_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          2097664     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           5130        dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,103,914\n",
      "Trainable params: 2,103,914\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = gcn_model()\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12160 samples, validate on 3040 samples\n",
      "Epoch 1/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 2.0373 - acc: 0.2416\n",
      "Epoch 00001: val_acc improved from -inf to 0.23355, saving model to gtzan_gcn.h5\n",
      "12160/12160 [==============================] - 10s 846us/sample - loss: 2.0372 - acc: 0.2419 - val_loss: 1.9683 - val_acc: 0.2336\n",
      "Epoch 2/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.9348 - acc: 0.2550\n",
      "Epoch 00002: val_acc improved from 0.23355 to 0.23980, saving model to gtzan_gcn.h5\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.9353 - acc: 0.2545 - val_loss: 1.9709 - val_acc: 0.2398\n",
      "Epoch 3/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.9143 - acc: 0.2645\n",
      "Epoch 00003: val_acc did not improve from 0.23980\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.9133 - acc: 0.2650 - val_loss: 1.9918 - val_acc: 0.2309\n",
      "Epoch 4/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.9097 - acc: 0.2691\n",
      "Epoch 00004: val_acc improved from 0.23980 to 0.24013, saving model to gtzan_gcn.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "12160/12160 [==============================] - 10s 813us/sample - loss: 1.9095 - acc: 0.2692 - val_loss: 1.9839 - val_acc: 0.2401\n",
      "Epoch 5/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.9010 - acc: 0.2656\n",
      "Epoch 00005: val_acc did not improve from 0.24013\n",
      "12160/12160 [==============================] - 10s 816us/sample - loss: 1.9014 - acc: 0.2651 - val_loss: 1.9877 - val_acc: 0.2378\n",
      "Epoch 6/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8993 - acc: 0.2628\n",
      "Epoch 00006: val_acc did not improve from 0.24013\n",
      "12160/12160 [==============================] - 10s 817us/sample - loss: 1.8997 - acc: 0.2628 - val_loss: 2.0141 - val_acc: 0.2280\n",
      "Epoch 7/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8926 - acc: 0.2735\n",
      "Epoch 00007: val_acc did not improve from 0.24013\n",
      "12160/12160 [==============================] - 10s 817us/sample - loss: 1.8921 - acc: 0.2741 - val_loss: 2.0273 - val_acc: 0.2273\n",
      "Epoch 8/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8945 - acc: 0.2705\n",
      "Epoch 00008: val_acc did not improve from 0.24013\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "12160/12160 [==============================] - 10s 814us/sample - loss: 1.8944 - acc: 0.2706 - val_loss: 1.9837 - val_acc: 0.2296\n",
      "Epoch 9/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8853 - acc: 0.2743\n",
      "Epoch 00009: val_acc did not improve from 0.24013\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8854 - acc: 0.2742 - val_loss: 2.0194 - val_acc: 0.2342\n",
      "Epoch 10/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8889 - acc: 0.2746\n",
      "Epoch 00010: val_acc did not improve from 0.24013\n",
      "12160/12160 [==============================] - 10s 810us/sample - loss: 1.8892 - acc: 0.2743 - val_loss: 1.9955 - val_acc: 0.2141\n",
      "Epoch 11/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8881 - acc: 0.2759\n",
      "Epoch 00011: val_acc did not improve from 0.24013\n",
      "12160/12160 [==============================] - 10s 814us/sample - loss: 1.8880 - acc: 0.2762 - val_loss: 2.0030 - val_acc: 0.2365\n",
      "Epoch 12/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8812 - acc: 0.2736\n",
      "Epoch 00012: val_acc improved from 0.24013 to 0.24803, saving model to gtzan_gcn.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8822 - acc: 0.2734 - val_loss: 2.0016 - val_acc: 0.2480\n",
      "Epoch 13/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8828 - acc: 0.2756\n",
      "Epoch 00013: val_acc did not improve from 0.24803\n",
      "12160/12160 [==============================] - 10s 813us/sample - loss: 1.8825 - acc: 0.2757 - val_loss: 1.9934 - val_acc: 0.2280\n",
      "Epoch 14/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8797 - acc: 0.2723\n",
      "Epoch 00014: val_acc did not improve from 0.24803\n",
      "12160/12160 [==============================] - 10s 806us/sample - loss: 1.8794 - acc: 0.2724 - val_loss: 2.0091 - val_acc: 0.2194\n",
      "Epoch 15/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8781 - acc: 0.2754\n",
      "Epoch 00015: val_acc improved from 0.24803 to 0.24967, saving model to gtzan_gcn.h5\n",
      "12160/12160 [==============================] - 10s 807us/sample - loss: 1.8785 - acc: 0.2749 - val_loss: 1.9843 - val_acc: 0.2497\n",
      "Epoch 16/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8773 - acc: 0.2765\n",
      "Epoch 00016: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0008145062311086804.\n",
      "12160/12160 [==============================] - 10s 816us/sample - loss: 1.8778 - acc: 0.2766 - val_loss: 1.9944 - val_acc: 0.2326\n",
      "Epoch 17/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8738 - acc: 0.2756\n",
      "Epoch 00017: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 812us/sample - loss: 1.8732 - acc: 0.2757 - val_loss: 2.0129 - val_acc: 0.2270\n",
      "Epoch 18/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8746 - acc: 0.2741\n",
      "Epoch 00018: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8753 - acc: 0.2742 - val_loss: 1.9844 - val_acc: 0.2349\n",
      "Epoch 19/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8732 - acc: 0.2735\n",
      "Epoch 00019: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 808us/sample - loss: 1.8735 - acc: 0.2731 - val_loss: 2.0247 - val_acc: 0.2257\n",
      "Epoch 20/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8725 - acc: 0.2769\n",
      "Epoch 00020: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0007737808919046074.\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8735 - acc: 0.2770 - val_loss: 1.9857 - val_acc: 0.2118\n",
      "Epoch 21/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8752 - acc: 0.2765\n",
      "Epoch 00021: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8742 - acc: 0.2771 - val_loss: 2.0096 - val_acc: 0.2368\n",
      "Epoch 22/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8700 - acc: 0.2786\n",
      "Epoch 00022: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 807us/sample - loss: 1.8700 - acc: 0.2789 - val_loss: 2.0078 - val_acc: 0.2204\n",
      "Epoch 23/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8681 - acc: 0.2795\n",
      "Epoch 00023: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 810us/sample - loss: 1.8684 - acc: 0.2791 - val_loss: 2.0092 - val_acc: 0.2102\n",
      "Epoch 24/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8676 - acc: 0.2833\n",
      "Epoch 00024: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000735091819660738.\n",
      "12160/12160 [==============================] - 10s 817us/sample - loss: 1.8674 - acc: 0.2835 - val_loss: 1.9930 - val_acc: 0.2303\n",
      "Epoch 25/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8672 - acc: 0.2825\n",
      "Epoch 00025: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 804us/sample - loss: 1.8669 - acc: 0.2828 - val_loss: 1.9826 - val_acc: 0.2217\n",
      "Epoch 26/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8659 - acc: 0.2827\n",
      "Epoch 00026: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8659 - acc: 0.2826 - val_loss: 1.9999 - val_acc: 0.2359\n",
      "Epoch 27/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8650 - acc: 0.2771\n",
      "Epoch 00027: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8659 - acc: 0.2768 - val_loss: 2.0311 - val_acc: 0.2286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8649 - acc: 0.2853\n",
      "Epoch 00028: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006983372120885178.\n",
      "12160/12160 [==============================] - 10s 808us/sample - loss: 1.8649 - acc: 0.2851 - val_loss: 2.0018 - val_acc: 0.2217\n",
      "Epoch 29/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8660 - acc: 0.2799\n",
      "Epoch 00029: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 807us/sample - loss: 1.8660 - acc: 0.2797 - val_loss: 2.0002 - val_acc: 0.2372\n",
      "Epoch 30/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8625 - acc: 0.2814\n",
      "Epoch 00030: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 810us/sample - loss: 1.8627 - acc: 0.2813 - val_loss: 2.0378 - val_acc: 0.2148\n",
      "Epoch 31/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8644 - acc: 0.2792\n",
      "Epoch 00031: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 810us/sample - loss: 1.8644 - acc: 0.2790 - val_loss: 2.0066 - val_acc: 0.2128\n",
      "Epoch 32/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8616 - acc: 0.2771\n",
      "Epoch 00032: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006634203542489559.\n",
      "12160/12160 [==============================] - 10s 815us/sample - loss: 1.8618 - acc: 0.2775 - val_loss: 2.0255 - val_acc: 0.2263\n",
      "Epoch 33/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8620 - acc: 0.2817\n",
      "Epoch 00033: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 803us/sample - loss: 1.8629 - acc: 0.2813 - val_loss: 2.0011 - val_acc: 0.2214\n",
      "Epoch 34/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8629 - acc: 0.2826\n",
      "Epoch 00034: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8626 - acc: 0.2826 - val_loss: 1.9933 - val_acc: 0.2329\n",
      "Epoch 35/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8622 - acc: 0.2832\n",
      "Epoch 00035: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 808us/sample - loss: 1.8617 - acc: 0.2834 - val_loss: 1.9843 - val_acc: 0.2342\n",
      "Epoch 36/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8655 - acc: 0.2786\n",
      "Epoch 00036: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006302493420662358.\n",
      "12160/12160 [==============================] - 10s 802us/sample - loss: 1.8661 - acc: 0.2781 - val_loss: 2.0121 - val_acc: 0.2257\n",
      "Epoch 37/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8602 - acc: 0.2831\n",
      "Epoch 00037: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 810us/sample - loss: 1.8596 - acc: 0.2836 - val_loss: 2.0195 - val_acc: 0.2428\n",
      "Epoch 38/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8581 - acc: 0.2847\n",
      "Epoch 00038: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 806us/sample - loss: 1.8579 - acc: 0.2847 - val_loss: 2.0278 - val_acc: 0.2151\n",
      "Epoch 39/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8587 - acc: 0.2831\n",
      "Epoch 00039: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 814us/sample - loss: 1.8584 - acc: 0.2832 - val_loss: 2.0138 - val_acc: 0.2263\n",
      "Epoch 40/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8584 - acc: 0.2812\n",
      "Epoch 00040: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005987368611386045.\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8579 - acc: 0.2815 - val_loss: 2.0225 - val_acc: 0.2220\n",
      "Epoch 41/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8572 - acc: 0.2841\n",
      "Epoch 00041: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 808us/sample - loss: 1.8565 - acc: 0.2840 - val_loss: 2.0386 - val_acc: 0.2253\n",
      "Epoch 42/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8557 - acc: 0.2827\n",
      "Epoch 00042: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8557 - acc: 0.2829 - val_loss: 2.0443 - val_acc: 0.2224\n",
      "Epoch 43/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8548 - acc: 0.2828\n",
      "Epoch 00043: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8550 - acc: 0.2828 - val_loss: 2.0311 - val_acc: 0.2181\n",
      "Epoch 44/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8589 - acc: 0.2836\n",
      "Epoch 00044: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0005688000208465382.\n",
      "12160/12160 [==============================] - 10s 801us/sample - loss: 1.8585 - acc: 0.2834 - val_loss: 2.0196 - val_acc: 0.2240\n",
      "Epoch 45/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8565 - acc: 0.2822\n",
      "Epoch 00045: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 816us/sample - loss: 1.8564 - acc: 0.2822 - val_loss: 2.0168 - val_acc: 0.2306\n",
      "Epoch 46/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8543 - acc: 0.2832\n",
      "Epoch 00046: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8546 - acc: 0.2830 - val_loss: 2.0093 - val_acc: 0.2174\n",
      "Epoch 47/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8540 - acc: 0.2878\n",
      "Epoch 00047: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 804us/sample - loss: 1.8546 - acc: 0.2876 - val_loss: 2.0133 - val_acc: 0.2109\n",
      "Epoch 48/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8535 - acc: 0.2858\n",
      "Epoch 00048: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0005403600225690752.\n",
      "12160/12160 [==============================] - 10s 806us/sample - loss: 1.8536 - acc: 0.2861 - val_loss: 1.9995 - val_acc: 0.2171\n",
      "Epoch 49/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8553 - acc: 0.2837\n",
      "Epoch 00049: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 817us/sample - loss: 1.8551 - acc: 0.2833 - val_loss: 2.0202 - val_acc: 0.2125\n",
      "Epoch 50/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8551 - acc: 0.2861\n",
      "Epoch 00050: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 808us/sample - loss: 1.8541 - acc: 0.2863 - val_loss: 2.0333 - val_acc: 0.2273\n",
      "Epoch 51/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8541 - acc: 0.2840\n",
      "Epoch 00051: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 813us/sample - loss: 1.8546 - acc: 0.2838 - val_loss: 2.0141 - val_acc: 0.2207\n",
      "Epoch 52/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8563 - acc: 0.2846\n",
      "Epoch 00052: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0005133419937919825.\n",
      "12160/12160 [==============================] - 10s 815us/sample - loss: 1.8557 - acc: 0.2850 - val_loss: 2.0298 - val_acc: 0.2263\n",
      "Epoch 53/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8548 - acc: 0.2799\n",
      "Epoch 00053: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 807us/sample - loss: 1.8543 - acc: 0.2802 - val_loss: 2.0129 - val_acc: 0.2283\n",
      "Epoch 54/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8553 - acc: 0.2808\n",
      "Epoch 00054: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 812us/sample - loss: 1.8548 - acc: 0.2811 - val_loss: 2.0254 - val_acc: 0.2332\n",
      "Epoch 55/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8549 - acc: 0.2831\n",
      "Epoch 00055: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 807us/sample - loss: 1.8550 - acc: 0.2828 - val_loss: 2.0102 - val_acc: 0.2240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8519 - acc: 0.2846\n",
      "Epoch 00056: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0004876748775132.\n",
      "12160/12160 [==============================] - 10s 810us/sample - loss: 1.8519 - acc: 0.2843 - val_loss: 2.0514 - val_acc: 0.2398\n",
      "Epoch 57/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8517 - acc: 0.2884\n",
      "Epoch 00057: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 804us/sample - loss: 1.8514 - acc: 0.2887 - val_loss: 2.0245 - val_acc: 0.2313\n",
      "Epoch 58/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8517 - acc: 0.2876\n",
      "Epoch 00058: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8520 - acc: 0.2876 - val_loss: 2.0378 - val_acc: 0.2174\n",
      "Epoch 59/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8523 - acc: 0.2858\n",
      "Epoch 00059: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 814us/sample - loss: 1.8527 - acc: 0.2852 - val_loss: 2.0084 - val_acc: 0.2115\n",
      "Epoch 60/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8536 - acc: 0.2818\n",
      "Epoch 00060: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00046329112810781223.\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8525 - acc: 0.2823 - val_loss: 2.0269 - val_acc: 0.2138\n",
      "Epoch 61/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8522 - acc: 0.2852\n",
      "Epoch 00061: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8511 - acc: 0.2858 - val_loss: 2.0288 - val_acc: 0.2164\n",
      "Epoch 62/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8517 - acc: 0.2878\n",
      "Epoch 00062: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 813us/sample - loss: 1.8511 - acc: 0.2880 - val_loss: 2.0544 - val_acc: 0.2178\n",
      "Epoch 63/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8518 - acc: 0.2832\n",
      "Epoch 00063: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8514 - acc: 0.2831 - val_loss: 2.0215 - val_acc: 0.2184\n",
      "Epoch 64/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8487 - acc: 0.2841\n",
      "Epoch 00064: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.00044012657308485355.\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8495 - acc: 0.2841 - val_loss: 2.0446 - val_acc: 0.2332\n",
      "Epoch 65/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8511 - acc: 0.2836\n",
      "Epoch 00065: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 800us/sample - loss: 1.8516 - acc: 0.2834 - val_loss: 2.0267 - val_acc: 0.2145\n",
      "Epoch 66/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8505 - acc: 0.2865\n",
      "Epoch 00066: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 806us/sample - loss: 1.8508 - acc: 0.2864 - val_loss: 2.0276 - val_acc: 0.2201\n",
      "Epoch 67/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8495 - acc: 0.2885\n",
      "Epoch 00067: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 815us/sample - loss: 1.8503 - acc: 0.2886 - val_loss: 2.0448 - val_acc: 0.2168\n",
      "Epoch 68/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8502 - acc: 0.2860\n",
      "Epoch 00068: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.00041812024719547477.\n",
      "12160/12160 [==============================] - 10s 801us/sample - loss: 1.8506 - acc: 0.2859 - val_loss: 2.0104 - val_acc: 0.2368\n",
      "Epoch 69/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8501 - acc: 0.2850\n",
      "Epoch 00069: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8498 - acc: 0.2850 - val_loss: 2.0024 - val_acc: 0.2066\n",
      "Epoch 70/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8485 - acc: 0.2914\n",
      "Epoch 00070: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 814us/sample - loss: 1.8486 - acc: 0.2913 - val_loss: 2.0159 - val_acc: 0.2260\n",
      "Epoch 71/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8505 - acc: 0.2850\n",
      "Epoch 00071: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 804us/sample - loss: 1.8495 - acc: 0.2853 - val_loss: 2.0375 - val_acc: 0.2112\n",
      "Epoch 72/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8491 - acc: 0.2873\n",
      "Epoch 00072: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.00039721422654110934.\n",
      "12160/12160 [==============================] - 10s 812us/sample - loss: 1.8494 - acc: 0.2874 - val_loss: 2.0396 - val_acc: 0.2224\n",
      "Epoch 73/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8488 - acc: 0.2851\n",
      "Epoch 00073: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8490 - acc: 0.2850 - val_loss: 2.0236 - val_acc: 0.2155\n",
      "Epoch 74/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8493 - acc: 0.2843\n",
      "Epoch 00074: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 812us/sample - loss: 1.8487 - acc: 0.2845 - val_loss: 2.0355 - val_acc: 0.2102\n",
      "Epoch 75/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8500 - acc: 0.2889\n",
      "Epoch 00075: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8491 - acc: 0.2887 - val_loss: 2.0329 - val_acc: 0.2270\n",
      "Epoch 76/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8487 - acc: 0.2861\n",
      "Epoch 00076: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.00037735351797891776.\n",
      "12160/12160 [==============================] - 10s 799us/sample - loss: 1.8486 - acc: 0.2863 - val_loss: 2.0459 - val_acc: 0.2194\n",
      "Epoch 77/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8488 - acc: 0.2855\n",
      "Epoch 00077: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8490 - acc: 0.2855 - val_loss: 2.0300 - val_acc: 0.2171\n",
      "Epoch 78/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8477 - acc: 0.2824\n",
      "Epoch 00078: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8475 - acc: 0.2828 - val_loss: 2.0291 - val_acc: 0.2424\n",
      "Epoch 79/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8481 - acc: 0.2846\n",
      "Epoch 00079: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 801us/sample - loss: 1.8479 - acc: 0.2845 - val_loss: 2.0528 - val_acc: 0.2178\n",
      "Epoch 80/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8474 - acc: 0.2860\n",
      "Epoch 00080: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.00035848583793267607.\n",
      "12160/12160 [==============================] - 10s 802us/sample - loss: 1.8478 - acc: 0.2856 - val_loss: 2.0301 - val_acc: 0.2184\n",
      "Epoch 81/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8465 - acc: 0.2835\n",
      "Epoch 00081: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8473 - acc: 0.2831 - val_loss: 2.0251 - val_acc: 0.2237\n",
      "Epoch 82/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8474 - acc: 0.2843\n",
      "Epoch 00082: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 809us/sample - loss: 1.8474 - acc: 0.2842 - val_loss: 2.0374 - val_acc: 0.2313\n",
      "Epoch 83/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8473 - acc: 0.2866\n",
      "Epoch 00083: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8472 - acc: 0.2866 - val_loss: 2.0353 - val_acc: 0.2214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8471 - acc: 0.2848\n",
      "Epoch 00084: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.00034056155709549785.\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8470 - acc: 0.2848 - val_loss: 2.0479 - val_acc: 0.2227\n",
      "Epoch 85/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8490 - acc: 0.2883\n",
      "Epoch 00085: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 808us/sample - loss: 1.8473 - acc: 0.2888 - val_loss: 2.0325 - val_acc: 0.2283\n",
      "Epoch 86/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8471 - acc: 0.2847\n",
      "Epoch 00086: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 806us/sample - loss: 1.8465 - acc: 0.2852 - val_loss: 2.0403 - val_acc: 0.2151\n",
      "Epoch 87/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8463 - acc: 0.2901\n",
      "Epoch 00087: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 806us/sample - loss: 1.8464 - acc: 0.2900 - val_loss: 2.0513 - val_acc: 0.2151\n",
      "Epoch 88/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8474 - acc: 0.2823\n",
      "Epoch 00088: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.00032353347924072293.\n",
      "12160/12160 [==============================] - 10s 811us/sample - loss: 1.8466 - acc: 0.2826 - val_loss: 2.0561 - val_acc: 0.2283\n",
      "Epoch 89/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8463 - acc: 0.2869\n",
      "Epoch 00089: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 807us/sample - loss: 1.8460 - acc: 0.2868 - val_loss: 2.0488 - val_acc: 0.2349\n",
      "Epoch 90/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8451 - acc: 0.2871\n",
      "Epoch 00090: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 810us/sample - loss: 1.8460 - acc: 0.2869 - val_loss: 2.0390 - val_acc: 0.2191\n",
      "Epoch 91/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8455 - acc: 0.2869\n",
      "Epoch 00091: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 807us/sample - loss: 1.8458 - acc: 0.2866 - val_loss: 2.0593 - val_acc: 0.2289\n",
      "Epoch 92/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8477 - acc: 0.2840\n",
      "Epoch 00092: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.00030735681357327847.\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8465 - acc: 0.2846 - val_loss: 2.0565 - val_acc: 0.2286\n",
      "Epoch 93/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8459 - acc: 0.2835\n",
      "Epoch 00093: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8458 - acc: 0.2831 - val_loss: 2.0449 - val_acc: 0.2181\n",
      "Epoch 94/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8458 - acc: 0.2853\n",
      "Epoch 00094: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 800us/sample - loss: 1.8459 - acc: 0.2855 - val_loss: 2.0371 - val_acc: 0.2326\n",
      "Epoch 95/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8459 - acc: 0.2874\n",
      "Epoch 00095: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 804us/sample - loss: 1.8459 - acc: 0.2873 - val_loss: 2.0511 - val_acc: 0.2240\n",
      "Epoch 96/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8458 - acc: 0.2861\n",
      "Epoch 00096: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.00029198898118920624.\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8455 - acc: 0.2861 - val_loss: 2.0356 - val_acc: 0.2191\n",
      "Epoch 97/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8451 - acc: 0.2879\n",
      "Epoch 00097: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 801us/sample - loss: 1.8452 - acc: 0.2873 - val_loss: 2.0511 - val_acc: 0.2224\n",
      "Epoch 98/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8453 - acc: 0.2875\n",
      "Epoch 00098: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 806us/sample - loss: 1.8451 - acc: 0.2874 - val_loss: 2.0581 - val_acc: 0.2164\n",
      "Epoch 99/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8448 - acc: 0.2879\n",
      "Epoch 00099: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 810us/sample - loss: 1.8446 - acc: 0.2876 - val_loss: 2.0330 - val_acc: 0.2273\n",
      "Epoch 100/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8463 - acc: 0.2884\n",
      "Epoch 00100: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.00027738953212974593.\n",
      "12160/12160 [==============================] - 10s 800us/sample - loss: 1.8464 - acc: 0.2882 - val_loss: 2.0518 - val_acc: 0.2191\n",
      "Epoch 101/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8450 - acc: 0.2867\n",
      "Epoch 00101: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 806us/sample - loss: 1.8444 - acc: 0.2875 - val_loss: 2.0510 - val_acc: 0.2092\n",
      "Epoch 102/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8457 - acc: 0.2881\n",
      "Epoch 00102: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 803us/sample - loss: 1.8456 - acc: 0.2879 - val_loss: 2.0422 - val_acc: 0.2289\n",
      "Epoch 103/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8445 - acc: 0.2854\n",
      "Epoch 00103: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 800us/sample - loss: 1.8449 - acc: 0.2850 - val_loss: 2.0515 - val_acc: 0.2158\n",
      "Epoch 104/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8453 - acc: 0.2890\n",
      "Epoch 00104: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 0.0002635200624354184.\n",
      "12160/12160 [==============================] - 10s 807us/sample - loss: 1.8443 - acc: 0.2892 - val_loss: 2.0560 - val_acc: 0.2112\n",
      "Epoch 105/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8443 - acc: 0.2829\n",
      "Epoch 00105: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 803us/sample - loss: 1.8444 - acc: 0.2830 - val_loss: 2.0636 - val_acc: 0.2204\n",
      "Epoch 106/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8430 - acc: 0.2864\n",
      "Epoch 00106: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 803us/sample - loss: 1.8432 - acc: 0.2864 - val_loss: 2.0567 - val_acc: 0.2184\n",
      "Epoch 107/1000\n",
      "12096/12160 [============================>.] - ETA: 0s - loss: 1.8445 - acc: 0.2845\n",
      "Epoch 00107: val_acc did not improve from 0.24967\n",
      "12160/12160 [==============================] - 10s 802us/sample - loss: 1.8448 - acc: 0.2845 - val_loss: 2.0647 - val_acc: 0.2184\n",
      "Epoch 108/1000\n",
      "12064/12160 [============================>.] - ETA: 0s - loss: 1.8455 - acc: 0.2871\n",
      "Epoch 00108: val_acc did not improve from 0.24967\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.0002503440482541919.\n",
      "12160/12160 [==============================] - 10s 805us/sample - loss: 1.8451 - acc: 0.2870 - val_loss: 2.0496 - val_acc: 0.2230\n",
      "Epoch 109/1000\n",
      "12128/12160 [============================>.] - ETA: 0s - loss: 1.8439 - acc: 0.2887"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-441a68fb4c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           callbacks=[\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mmc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m           ])\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Genre_classification/GNN/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Genre_classification/GNN/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Genre_classification/GNN/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    445\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Genre_classification/GNN/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Genre_classification/GNN/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3566\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3567\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3568\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/Genre_classification/GNN/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mc = ModelCheckpoint('gtzan_gcn.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "rl = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=3, verbose=1, mode='min', min_delta=0.0001, cooldown=2, min_lr=1e-5)\n",
    "callback_list = [mc,rl]\n",
    "\n",
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    callbacks=[mc,rl])\n",
    "\n",
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "eval_results = model.evaluate(X_test,\n",
    "                              y_test,\n",
    "                              batch_size=batch_size)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "      'Test acc: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confuison matrix\n",
    "preds = np.argmax(model.predict(X_test), axis = 1)\n",
    "y_orig = np.argmax(y_test, axis = 1)\n",
    "cm = confusion_matrix(preds, y_orig)\n",
    "\n",
    "keys = OrderedDict(sorted(genres.items(), key=lambda t:t[1])).keys()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, keys, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority voting\n",
    "preds = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "\n",
    "scores_songs = np.split(np.argmax(preds, axis=1), 300)\n",
    "scores_songs = [majority_vote(scores) for scores in scores_songs]\n",
    "\n",
    "label = np.split(np.argmax(y_test, axis=1), 300)\n",
    "label = [majority_vote(l) for l in label]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"majority voting system (acc) = {:.3f}\".format(accuracy_score(label, scores_songs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
