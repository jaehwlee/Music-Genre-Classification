{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from copy import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "TRAIN_FEATURES_PATH = '/home/jaehwlee/Genre_classification/GNN/feature_data/X_train_features.npy'\n",
    "VALID_FEATURES_PATH = '/home/jaehwlee/Genre_classification/GNN/feature_data/X_valid_features.npy'\n",
    "TEST_FEATURES_PATH = '/home/jaehwlee/Genre_classification/GNN/feature_data/X_test_features.npy'\n",
    "\n",
    "# Parameters\n",
    "l2_reg = 5e-4         # Regularization rate for l2\n",
    "learning_rate = 1e-3  # Learning rate for SGD\n",
    "batch_size = 128       # Batch size\n",
    "epochs = 100       # Number of training epochs\n",
    "es_patience = 10      # Patience fot early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# majority vote\n",
    "def majority_vote(scores):\n",
    "    values, counts = np.unique(scores,return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    return values[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "Loading dataset...\n",
      "============================================================================\n",
      "(12160, 256, 256, 1)\n",
      "(3040, 256, 256, 1)\n",
      "(3800, 256, 256, 1)\n",
      "(12160, 10)\n",
      "(3040, 10)\n",
      "(3800, 10)\n",
      "============================================================================\n",
      "complete!\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print('============================================================================')\n",
    "print('Loading dataset...')\n",
    "print('============================================================================')\n",
    "X_train = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/X_train.npy')\n",
    "X_valid = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/X_valid.npy')\n",
    "X_test = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/X_test.npy')\n",
    "y_train = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/y_train.npy')\n",
    "y_valid = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/y_valid.npy')\n",
    "y_test = np.load('/home/jaehwlee/Genre_classification/GNN/mel_data/y_test.npy')\n",
    "\n",
    "song_samples = 660000\n",
    "genres = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4, \n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print('============================================================================')\n",
    "print('complete!')\n",
    "print('============================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract_model():\n",
    "    input_mel = Input(shape=(256,256,1))\n",
    "\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_mel)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    out = Dense(10, activation='softmax')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=input_mel, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12160 samples, validate on 3040 samples\n",
      "Epoch 1/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 1.9781 - acc: 0.2471\n",
      "Epoch 00001: val_acc improved from -inf to 0.38125, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 20s 2ms/sample - loss: 1.9743 - acc: 0.2491 - val_loss: 1.7637 - val_acc: 0.3812\n",
      "Epoch 2/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 1.5002 - acc: 0.4707\n",
      "Epoch 00002: val_acc improved from 0.38125 to 0.49737, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 1.5007 - acc: 0.4720 - val_loss: 1.5482 - val_acc: 0.4974\n",
      "Epoch 3/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 1.2681 - acc: 0.5694\n",
      "Epoch 00003: val_acc improved from 0.49737 to 0.58257, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 1.2673 - acc: 0.5699 - val_loss: 1.2987 - val_acc: 0.5826\n",
      "Epoch 4/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 1.0440 - acc: 0.6473\n",
      "Epoch 00004: val_acc improved from 0.58257 to 0.60033, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 1.0427 - acc: 0.6475 - val_loss: 1.2917 - val_acc: 0.6003\n",
      "Epoch 5/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.8808 - acc: 0.7059\n",
      "Epoch 00005: val_acc improved from 0.60033 to 0.63750, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.8832 - acc: 0.7055 - val_loss: 1.3310 - val_acc: 0.6375\n",
      "Epoch 6/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.7476 - acc: 0.7555\n",
      "Epoch 00006: val_acc improved from 0.63750 to 0.66217, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.7489 - acc: 0.7551 - val_loss: 1.6904 - val_acc: 0.6622\n",
      "Epoch 7/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.7813\n",
      "Epoch 00007: val_acc improved from 0.66217 to 0.66513, saving model to 128test.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.6666 - acc: 0.7812 - val_loss: 1.4200 - val_acc: 0.6651\n",
      "Epoch 8/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.5891 - acc: 0.8088\n",
      "Epoch 00008: val_acc improved from 0.66513 to 0.68783, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.5895 - acc: 0.8086 - val_loss: 1.4910 - val_acc: 0.6878\n",
      "Epoch 9/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.8250\n",
      "Epoch 00009: val_acc improved from 0.68783 to 0.70428, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.5334 - acc: 0.8258 - val_loss: 1.3319 - val_acc: 0.7043\n",
      "Epoch 10/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.4961 - acc: 0.8389\n",
      "Epoch 00010: val_acc did not improve from 0.70428\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.4962 - acc: 0.8391 - val_loss: 1.3485 - val_acc: 0.6997\n",
      "Epoch 11/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.8565\n",
      "Epoch 00011: val_acc improved from 0.70428 to 0.72072, saving model to 128test.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.4503 - acc: 0.8566 - val_loss: 1.4336 - val_acc: 0.7207\n",
      "Epoch 12/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8689\n",
      "Epoch 00012: val_acc improved from 0.72072 to 0.73158, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.4068 - acc: 0.8691 - val_loss: 1.4429 - val_acc: 0.7316\n",
      "Epoch 13/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8858\n",
      "Epoch 00013: val_acc did not improve from 0.73158\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.3491 - acc: 0.8858 - val_loss: 1.4890 - val_acc: 0.6997\n",
      "Epoch 14/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8954\n",
      "Epoch 00014: val_acc did not improve from 0.73158\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.3245 - acc: 0.8946 - val_loss: 1.4631 - val_acc: 0.7250\n",
      "Epoch 15/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9040\n",
      "Epoch 00015: val_acc did not improve from 0.73158\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.2968 - acc: 0.9041 - val_loss: 1.4493 - val_acc: 0.7128\n",
      "Epoch 16/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9220\n",
      "Epoch 00016: val_acc improved from 0.73158 to 0.74375, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.2451 - acc: 0.9220 - val_loss: 1.6019 - val_acc: 0.7437\n",
      "Epoch 17/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9083\n",
      "Epoch 00017: val_acc did not improve from 0.74375\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.2816 - acc: 0.9085 - val_loss: 1.5890 - val_acc: 0.7395\n",
      "Epoch 18/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9221\n",
      "Epoch 00018: val_acc did not improve from 0.74375\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.2480 - acc: 0.9221 - val_loss: 1.6412 - val_acc: 0.7385\n",
      "Epoch 19/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9364\n",
      "Epoch 00019: val_acc did not improve from 0.74375\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0008145062311086804.\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1943 - acc: 0.9365 - val_loss: 2.1388 - val_acc: 0.7217\n",
      "Epoch 20/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9363\n",
      "Epoch 00020: val_acc did not improve from 0.74375\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1969 - acc: 0.9362 - val_loss: 2.1760 - val_acc: 0.7266\n",
      "Epoch 21/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9447\n",
      "Epoch 00021: val_acc did not improve from 0.74375\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1750 - acc: 0.9450 - val_loss: 2.2194 - val_acc: 0.7273\n",
      "Epoch 22/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9430\n",
      "Epoch 00022: val_acc did not improve from 0.74375\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1841 - acc: 0.9432 - val_loss: 2.1693 - val_acc: 0.7023\n",
      "Epoch 23/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9520\n",
      "Epoch 00023: val_acc did not improve from 0.74375\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0007737808919046074.\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1515 - acc: 0.9522 - val_loss: 2.2949 - val_acc: 0.7191\n",
      "Epoch 24/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9625\n",
      "Epoch 00024: val_acc did not improve from 0.74375\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1199 - acc: 0.9627 - val_loss: 2.9107 - val_acc: 0.7319\n",
      "Epoch 25/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9397\n",
      "Epoch 00025: val_acc did not improve from 0.74375\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1942 - acc: 0.9395 - val_loss: 2.2681 - val_acc: 0.7391\n",
      "Epoch 26/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9617\n",
      "Epoch 00026: val_acc did not improve from 0.74375\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1212 - acc: 0.9613 - val_loss: 2.6647 - val_acc: 0.7138\n",
      "Epoch 27/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9539\n",
      "Epoch 00027: val_acc did not improve from 0.74375\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.000735091819660738.\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1491 - acc: 0.9542 - val_loss: 3.0582 - val_acc: 0.7237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9683\n",
      "Epoch 00028: val_acc improved from 0.74375 to 0.75362, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.0993 - acc: 0.9683 - val_loss: 2.6356 - val_acc: 0.7536\n",
      "Epoch 29/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9675\n",
      "Epoch 00029: val_acc did not improve from 0.75362\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1046 - acc: 0.9675 - val_loss: 2.8115 - val_acc: 0.7033\n",
      "Epoch 30/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9712\n",
      "Epoch 00030: val_acc did not improve from 0.75362\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.0930 - acc: 0.9713 - val_loss: 2.7132 - val_acc: 0.7513\n",
      "Epoch 31/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9722\n",
      "Epoch 00031: val_acc did not improve from 0.75362\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006983372120885178.\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.0878 - acc: 0.9721 - val_loss: 2.7213 - val_acc: 0.7382\n",
      "Epoch 32/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9762\n",
      "Epoch 00032: val_acc improved from 0.75362 to 0.75592, saving model to 128test.h5\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.0824 - acc: 0.9763 - val_loss: 3.1249 - val_acc: 0.7559\n",
      "Epoch 33/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9781\n",
      "Epoch 00033: val_acc did not improve from 0.75592\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.0729 - acc: 0.9782 - val_loss: 2.9463 - val_acc: 0.7230\n",
      "Epoch 34/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9649\n",
      "Epoch 00034: val_acc did not improve from 0.75592\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.1164 - acc: 0.9646 - val_loss: 2.4673 - val_acc: 0.7408\n",
      "Epoch 35/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9808\n",
      "Epoch 00035: val_acc improved from 0.75592 to 0.75888, saving model to 128test.h5\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006634203542489559.\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.0647 - acc: 0.9807 - val_loss: 3.1943 - val_acc: 0.7589\n",
      "Epoch 36/100\n",
      "12032/12160 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9837\n",
      "Epoch 00036: val_acc did not improve from 0.75888\n",
      "12160/12160 [==============================] - 17s 1ms/sample - loss: 0.0550 - acc: 0.9838 - val_loss: 3.5629 - val_acc: 0.7464\n",
      "Epoch 37/100\n",
      " 4096/12160 [=========>....................] - ETA: 10s - loss: 0.0528 - acc: 0.9824"
     ]
    }
   ],
   "source": [
    "model = feature_extract_model()\n",
    "validation_data = (X_valid, y_valid)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "mc = ModelCheckpoint('128test.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "rl = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=3, verbose=1, mode='min', min_delta=0.0001, cooldown=2, min_lr=1e-5)\n",
    "callback_list = [mc,rl]\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=validation_data,\n",
    "          epochs=epochs,\n",
    "          callbacks=[\n",
    "              mc,rl\n",
    "          ])\n",
    "model.save('128test.h5')\n",
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "eval_results = model.evaluate(X_test,\n",
    "                              y_test,\n",
    "                              batch_size=batch_size)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "      'Test acc: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"128test.h5\")\n",
    "backbone_model = Model(inputs=model.input, outputs=model.get_layer('global_average_pooling2d').output)\n",
    "X_train_features = backbone_model.predict(X_train)\n",
    "X_valid_features = backbone_model.predict(X_valid)\n",
    "X_test_features = backbone_model.predict(X_test)\n",
    "np.save(TRAIN_FEATURES_PATH, X_train_features)\n",
    "np.save(VALID_FEATURES_PATH, X_valid_features)\n",
    "np.save(TEST_FEATURES_PATH, X_test_features)\n",
    "\n",
    "print('save complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
